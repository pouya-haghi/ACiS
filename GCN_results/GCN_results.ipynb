{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfba76f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determing the packet length for each rank\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "# file_name = 'GCN_matrices/Cora_A.txt'\n",
    "num_rank = 96\n",
    "def get_packetLength_per_rank(num_rank): # This is the number of elements\n",
    "    directory = 'GCN_matrices'\n",
    "    dict_packet_length = {}\n",
    "    dict_ratio = {}\n",
    "    for filenm in os.listdir(directory): # iterate over all files\n",
    "        file_name = os.path.join(directory, filenm)\n",
    "        if os.path.isfile(file_name): # if it is a file\n",
    "            splitted_file_name = file_name.split(\"/\")[1].split(\"_\")\n",
    "            dataset = splitted_file_name[0]\n",
    "#             print(file_name)\n",
    "            bucket_list = [0 for g in range(num_rank)] # total: both diagonal and off-diagonal\n",
    "            bucket_list_diagonal = [0 for g in range(num_rank)] # only diagonal\n",
    "            with open(file_name) as f:\n",
    "                line1 = f.readline() \n",
    "                line2 = int(f.readline())\n",
    "                lines = f.readlines()\n",
    "                num_row = int(line1.split()[0])\n",
    "                num_col = int(line1.split()[1])\n",
    "                bucket_length = int(num_row/num_rank)\n",
    "                for i in range(line2):\n",
    "#                     print(lines[i])\n",
    "#                     print(i)\n",
    "#                     print(row)\n",
    "                    row = int(lines[i].split()[0])\n",
    "                    col = int(lines[i].split()[1])\n",
    "                    bucket = int(row/bucket_length)\n",
    "                    if bucket > num_rank - 1: # if we are out of range as a result of rounding bucket_length \\\\\n",
    "                        # just add these outliers to the last rank\n",
    "                        bucket = num_rank - 1\n",
    "                    bucket_list[bucket] += 1\n",
    "                    if col >= bucket*bucket_length and col < (bucket +1) * bucket_length:\n",
    "                        bucket_list_diagonal[bucket] += 1\n",
    "            ratio_diag = np.array(bucket_list_diagonal)/np.array(bucket_list)\n",
    "            # ratio of diag over total (both diagonal and off-diagonal)\n",
    "            if splitted_file_name[1] == 'A.txt': # A matrix is being used for both layers\n",
    "                key1 = dataset + '_L1_AXW'\n",
    "                key2 = dataset + '_L2_AXW'\n",
    "                dict_packet_length[key1] = bucket_list\n",
    "                dict_packet_length[key2] = bucket_list\n",
    "                dict_ratio[key1] = ratio_diag\n",
    "                dict_ratio[key2] = ratio_diag\n",
    "            elif splitted_file_name[1] == 'feat': \n",
    "                if splitted_file_name[2] == 'L1.txt':\n",
    "                    key = dataset + '_L1_XW'\n",
    "                    dict_packet_length[key] = bucket_list\n",
    "                    dict_ratio[key] = ratio_diag\n",
    "                elif splitted_file_name[2] == 'L2.txt':\n",
    "                    key = dataset + '_L2_XW'\n",
    "                    dict_packet_length[key] = bucket_list\n",
    "                    dict_ratio[key] = ratio_diag\n",
    "                else:\n",
    "                    print('unsupported file name')\n",
    "            else:\n",
    "                print('unsupported file name')\n",
    "    return dict_packet_length, dict_ratio\n",
    "#     return dict_packet_length\n",
    "# print(get_packetLength_per_rank(file_name, num_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "726f3273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "{'Cora_L2_XW': [379, 366, 339, 392, 357, 366, 370, 390, 379, 385, 379, 369, 362, 382, 355, 381, 386, 370, 387, 368, 368, 354, 374, 385, 389, 385, 381, 367, 384, 378, 379, 388, 386, 377, 387, 375, 391, 381, 384, 381, 390, 381, 372, 352, 350, 379, 385, 373, 380, 380, 363, 369, 371, 380, 381, 390, 384, 384, 388, 374, 373, 392, 376, 363, 387, 385, 387, 382, 393, 382, 361, 350, 378, 389, 383, 389, 385, 387, 387, 386, 384, 389, 388, 395, 396, 365, 351, 387, 392, 371, 374, 387, 386, 388, 385, 643], 'Cora_L1_AXW': [94, 61, 50, 28, 80, 45, 32, 29, 62, 56, 71, 90, 74, 71, 87, 50, 62, 57, 65, 83, 77, 73, 67, 71, 67, 106, 88, 109, 111, 86, 85, 83, 96, 81, 74, 80, 82, 103, 103, 94, 132, 94, 88, 101, 130, 95, 110, 164, 84, 141, 316, 149, 112, 87, 60, 72, 96, 127, 132, 114, 108, 85, 156, 120, 118, 115, 91, 116, 192, 153, 150, 218, 141, 164, 130, 282, 153, 88, 113, 221, 150, 127, 111, 142, 77, 131, 97, 164, 76, 102, 172, 147, 233, 209, 161, 156], 'Cora_L2_AXW': [94, 61, 50, 28, 80, 45, 32, 29, 62, 56, 71, 90, 74, 71, 87, 50, 62, 57, 65, 83, 77, 73, 67, 71, 67, 106, 88, 109, 111, 86, 85, 83, 96, 81, 74, 80, 82, 103, 103, 94, 132, 94, 88, 101, 130, 95, 110, 164, 84, 141, 316, 149, 112, 87, 60, 72, 96, 127, 132, 114, 108, 85, 156, 120, 118, 115, 91, 116, 192, 153, 150, 218, 141, 164, 130, 282, 153, 88, 113, 221, 150, 127, 111, 142, 77, 131, 97, 164, 76, 102, 172, 147, 233, 209, 161, 156], 'Cora_L1_XW': [447, 424, 465, 499, 473, 451, 493, 499, 475, 536, 461, 543, 544, 484, 562, 479, 505, 476, 517, 553, 505, 452, 530, 507, 501, 547, 549, 502, 554, 444, 524, 485, 464, 559, 507, 472, 528, 492, 530, 539, 521, 513, 521, 520, 570, 476, 541, 553, 522, 520, 452, 512, 441, 565, 429, 438, 506, 579, 553, 583, 551, 512, 520, 496, 545, 562, 510, 539, 516, 523, 545, 514, 559, 502, 511, 477, 541, 531, 500, 538, 524, 520, 528, 536, 490, 578, 522, 478, 460, 377, 465, 537, 486, 495, 471, 865], 'Pubmed_L1_AXW': [291, 282, 294, 346, 484, 328, 404, 339, 488, 334, 293, 283, 348, 332, 298, 306, 248, 283, 354, 529, 516, 384, 310, 438, 494, 382, 353, 382, 359, 367, 378, 811, 514, 511, 385, 325, 451, 482, 453, 674, 539, 948, 588, 984, 1027, 943, 949, 1182, 715, 903, 1115, 1498, 1582, 1698, 1888, 1194, 616, 721, 1572, 1535, 2063, 1054, 1545, 947, 1806, 1450, 1326, 1397, 1268, 712, 658, 559, 946, 817, 613, 681, 875, 711, 629, 1023, 2132, 2451, 1063, 1698, 1712, 1916, 1717, 1209, 1046, 1227, 763, 1248, 3152, 3900, 2552, 1755], 'Pubmed_L2_AXW': [291, 282, 294, 346, 484, 328, 404, 339, 488, 334, 293, 283, 348, 332, 298, 306, 248, 283, 354, 529, 516, 384, 310, 438, 494, 382, 353, 382, 359, 367, 378, 811, 514, 511, 385, 325, 451, 482, 453, 674, 539, 948, 588, 984, 1027, 943, 949, 1182, 715, 903, 1115, 1498, 1582, 1698, 1888, 1194, 616, 721, 1572, 1535, 2063, 1054, 1545, 947, 1806, 1450, 1326, 1397, 1268, 712, 658, 559, 946, 817, 613, 681, 875, 711, 629, 1023, 2132, 2451, 1063, 1698, 1712, 1916, 1717, 1209, 1046, 1227, 763, 1248, 3152, 3900, 2552, 1755], 'Reddit_L1_AXW': [27518, 50353, 100360, 199860, 130866, 357853, 220019, 251534, 137953, 408263, 163071, 276174, 424518, 281298, 233707, 417268, 360310, 339434, 264203, 328859, 373606, 332537, 650338, 427212, 464835, 482269, 442200, 662724, 702616, 440659, 715745, 674788, 602399, 770236, 607579, 754658, 791600, 884487, 841500, 813560, 787988, 781995, 790707, 907681, 1189788, 916255, 1138706, 952855, 1434373, 913756, 1478451, 1300135, 1529654, 1267195, 1263417, 1507581, 1304800, 1392949, 1364164, 1386742, 1224429, 1894086, 1311282, 1567318, 2063212, 1781939, 1884523, 1718018, 1483451, 1718936, 2044181, 1602383, 1588199, 1735659, 1764731, 1659572, 1736398, 1781552, 1724281, 1592748, 1490537, 2034776, 1788721, 2342522, 1831381, 2128723, 2103448, 1725366, 2543553, 1820556, 2010185, 2176257, 2079963, 2003054, 9290568, 2351223], 'Reddit_L2_AXW': [27518, 50353, 100360, 199860, 130866, 357853, 220019, 251534, 137953, 408263, 163071, 276174, 424518, 281298, 233707, 417268, 360310, 339434, 264203, 328859, 373606, 332537, 650338, 427212, 464835, 482269, 442200, 662724, 702616, 440659, 715745, 674788, 602399, 770236, 607579, 754658, 791600, 884487, 841500, 813560, 787988, 781995, 790707, 907681, 1189788, 916255, 1138706, 952855, 1434373, 913756, 1478451, 1300135, 1529654, 1267195, 1263417, 1507581, 1304800, 1392949, 1364164, 1386742, 1224429, 1894086, 1311282, 1567318, 2063212, 1781939, 1884523, 1718018, 1483451, 1718936, 2044181, 1602383, 1588199, 1735659, 1764731, 1659572, 1736398, 1781552, 1724281, 1592748, 1490537, 2034776, 1788721, 2342522, 1831381, 2128723, 2103448, 1725366, 2543553, 1820556, 2010185, 2176257, 2079963, 2003054, 9290568, 2351223], 'Citeseer_L2_XW': [361, 368, 368, 341, 351, 367, 360, 374, 370, 372, 371, 356, 367, 369, 371, 374, 366, 354, 360, 359, 373, 374, 374, 374, 374, 360, 370, 368, 368, 374, 371, 363, 372, 374, 367, 360, 364, 357, 360, 372, 367, 367, 368, 359, 371, 365, 363, 370, 373, 364, 364, 360, 370, 362, 361, 368, 371, 374, 361, 370, 365, 360, 372, 372, 367, 362, 368, 367, 364, 369, 357, 372, 358, 360, 369, 371, 360, 366, 366, 370, 366, 358, 353, 363, 360, 363, 360, 364, 350, 358, 357, 357, 361, 355, 360, 1036], 'Citeseer_L1_XW': [1135, 1070, 1084, 1095, 1095, 1073, 1145, 1008, 1073, 1041, 1085, 1080, 1029, 1008, 1094, 1063, 999, 1106, 1080, 1034, 1096, 1067, 964, 1058, 1085, 993, 1054, 1062, 995, 1025, 1085, 1103, 1038, 985, 998, 1002, 1072, 1096, 1083, 1127, 1068, 1042, 1075, 1152, 1133, 1107, 1078, 1076, 1053, 1072, 1053, 1138, 1024, 1029, 1053, 1066, 1051, 1031, 1080, 1084, 1090, 1127, 1086, 1086, 1074, 1116, 1096, 1121, 1114, 1110, 1066, 1106, 1102, 1021, 1108, 1154, 1108, 1047, 1075, 1072, 1115, 1012, 1091, 1079, 1112, 1131, 1145, 1097, 1046, 1138, 1139, 1017, 1107, 1148, 1068, 2991], 'Citeseer_L1_AXW': [64, 63, 64, 50, 53, 40, 59, 41, 47, 39, 49, 42, 44, 47, 43, 56, 40, 56, 53, 59, 50, 50, 65, 39, 61, 62, 74, 41, 44, 51, 45, 51, 41, 54, 46, 66, 60, 56, 73, 107, 77, 54, 93, 132, 63, 83, 121, 83, 63, 111, 62, 101, 68, 77, 104, 94, 171, 77, 109, 74, 90, 152, 134, 99, 159, 92, 96, 132, 230, 321, 105, 108, 75, 175, 156, 127, 243, 115, 142, 179, 184, 155, 199, 120, 129, 87, 141, 92, 117, 189, 115, 102, 86, 156, 104, 360], 'Citeseer_L2_AXW': [64, 63, 64, 50, 53, 40, 59, 41, 47, 39, 49, 42, 44, 47, 43, 56, 40, 56, 53, 59, 50, 50, 65, 39, 61, 62, 74, 41, 44, 51, 45, 51, 41, 54, 46, 66, 60, 56, 73, 107, 77, 54, 93, 132, 63, 83, 121, 83, 63, 111, 62, 101, 68, 77, 104, 94, 171, 77, 109, 74, 90, 152, 134, 99, 159, 92, 96, 132, 230, 321, 105, 108, 75, 175, 156, 127, 243, 115, 142, 179, 184, 155, 199, 120, 129, 87, 141, 92, 117, 189, 115, 102, 86, 156, 104, 360], 'Pubmed_L2_XW': [2620, 2633, 2611, 2626, 2625, 2570, 2627, 2631, 2630, 2623, 2657, 2637, 2636, 2633, 2585, 2626, 2618, 2612, 2616, 2642, 2625, 2606, 2569, 2518, 2604, 2599, 2569, 2624, 2652, 2627, 2612, 2639, 2549, 2615, 2563, 2563, 2571, 2541, 2556, 2606, 2568, 2545, 2583, 2599, 2591, 2574, 2536, 2552, 2523, 2569, 2603, 2598, 2560, 2490, 2365, 2458, 2581, 2603, 2505, 2555, 2577, 2552, 2542, 2515, 2384, 2411, 2446, 2532, 2553, 2553, 2498, 2604, 2593, 2574, 2516, 2514, 2555, 2569, 2586, 2495, 2368, 2426, 2596, 2563, 2422, 2326, 2472, 2581, 2541, 2494, 2433, 2272, 2473, 2415, 2431, 2891], 'Reddit_L2_XW': [24210, 20194, 23998, 31047, 23420, 38815, 24392, 22809, 8084, 10979, 8069, 19813, 35236, 10724, 7619, 5899, 5335, 8007, 5219, 3730, 6136, 5472, 8067, 6420, 7008, 7001, 3299, 2359, 4504, 5105, 4288, 2984, 3297, 6539, 2897, 3471, 3604, 4861, 5975, 6375, 1875, 3510, 2689, 4417, 6328, 4667, 1960, 7639, 3503, 5606, 1942, 6836, 7236, 2613, 4246, 1463, 5833, 2642, 2557, 2984, 4388, 1629, 3027, 1792, 1340, 1830, 4777, 4103, 3068, 3936, 1739, 4377, 756, 1406, 1687, 430, 2485, 1314, 1909, 2211, 1353, 1566, 6465, 1498, 1262, 1351, 1248, 5930, 2087, 8682, 1621, 1786, 911, 1819, 813, 645], 'Pubmed_L1_XW': [8830, 8654, 9375, 8915, 8940, 9155, 9545, 10235, 9012, 9942, 9095, 10305, 8729, 10448, 10345, 10088, 10732, 10536, 9576, 9484, 9425, 9357, 9417, 9687, 10194, 11092, 10449, 10610, 10114, 10661, 10754, 10152, 10743, 10869, 10868, 9727, 9424, 9609, 9202, 11117, 11441, 10246, 10475, 10030, 10188, 10386, 10433, 10361, 9892, 9124, 10139, 9311, 9423, 9401, 9082, 9722, 11782, 11097, 10554, 11048, 10034, 10144, 11061, 11156, 9445, 9543, 9206, 11196, 11718, 11216, 10787, 10774, 10868, 10795, 10800, 12458, 11879, 12458, 13012, 10472, 9318, 9076, 10124, 9464, 9655, 9924, 12207, 11618, 11878, 11672, 10513, 9178, 11315, 10730, 11118, 11672], 'Reddit_L1_XW': [8830, 8654, 9375, 8915, 8940, 9155, 9545, 10235, 9012, 9942, 9095, 10305, 8729, 10448, 10345, 10088, 10732, 10536, 9576, 9484, 9425, 9357, 9417, 9687, 10194, 11092, 10449, 10610, 10114, 10661, 10754, 10152, 10743, 10869, 10868, 9727, 9424, 9609, 9202, 11117, 11441, 10246, 10475, 10030, 10188, 10386, 10433, 10361, 9892, 9124, 10139, 9311, 9423, 9401, 9082, 9722, 11782, 11097, 10554, 11048, 10034, 10144, 11061, 11156, 9445, 9543, 9206, 11196, 11718, 11216, 10787, 10774, 10868, 10795, 10800, 12458, 11879, 12458, 13012, 10472, 9318, 9076, 10124, 9464, 9655, 9924, 12207, 11618, 11878, 11672, 10513, 9178, 11315, 10730, 11118, 11672]}\n",
      "-------------------------------------------\n",
      "{'Cora_L2_XW': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Cora_L1_AXW': array([1.        , 0.98360656, 0.96      , 0.92857143, 0.9       ,\n",
      "       0.84444444, 1.        , 0.89655172, 0.41935484, 0.10714286,\n",
      "       0.16901408, 0.17777778, 0.08108108, 0.11267606, 0.18390805,\n",
      "       0.08      , 0.12903226, 0.07017544, 0.18461538, 0.09638554,\n",
      "       0.1038961 , 0.16438356, 0.23880597, 0.11267606, 0.05970149,\n",
      "       0.0754717 , 0.        , 0.09174312, 0.09009009, 0.06976744,\n",
      "       0.07058824, 0.1686747 , 0.25      , 0.09876543, 0.05405405,\n",
      "       0.        , 0.09756098, 0.09708738, 0.17475728, 0.14893617,\n",
      "       0.39393939, 0.08510638, 0.15909091, 0.21782178, 0.12307692,\n",
      "       0.06315789, 0.01818182, 0.25609756, 0.04761905, 0.02836879,\n",
      "       0.11392405, 0.06711409, 0.03571429, 0.04597701, 0.03333333,\n",
      "       0.19444444, 0.1875    , 0.11023622, 0.06060606, 0.07017544,\n",
      "       0.31481481, 0.09411765, 0.14102564, 0.15      , 0.06779661,\n",
      "       0.05217391, 0.08791209, 0.06896552, 0.08333333, 0.14379085,\n",
      "       0.16      , 0.0733945 , 0.05673759, 0.07317073, 0.09230769,\n",
      "       0.02836879, 0.09150327, 0.        , 0.12389381, 0.09954751,\n",
      "       0.09333333, 0.12598425, 0.10810811, 0.07042254, 0.        ,\n",
      "       0.01526718, 0.08247423, 0.12195122, 0.02631579, 0.35294118,\n",
      "       0.15116279, 0.08163265, 0.06866953, 0.07655502, 0.31055901,\n",
      "       0.43589744]), 'Cora_L2_AXW': array([1.        , 0.98360656, 0.96      , 0.92857143, 0.9       ,\n",
      "       0.84444444, 1.        , 0.89655172, 0.41935484, 0.10714286,\n",
      "       0.16901408, 0.17777778, 0.08108108, 0.11267606, 0.18390805,\n",
      "       0.08      , 0.12903226, 0.07017544, 0.18461538, 0.09638554,\n",
      "       0.1038961 , 0.16438356, 0.23880597, 0.11267606, 0.05970149,\n",
      "       0.0754717 , 0.        , 0.09174312, 0.09009009, 0.06976744,\n",
      "       0.07058824, 0.1686747 , 0.25      , 0.09876543, 0.05405405,\n",
      "       0.        , 0.09756098, 0.09708738, 0.17475728, 0.14893617,\n",
      "       0.39393939, 0.08510638, 0.15909091, 0.21782178, 0.12307692,\n",
      "       0.06315789, 0.01818182, 0.25609756, 0.04761905, 0.02836879,\n",
      "       0.11392405, 0.06711409, 0.03571429, 0.04597701, 0.03333333,\n",
      "       0.19444444, 0.1875    , 0.11023622, 0.06060606, 0.07017544,\n",
      "       0.31481481, 0.09411765, 0.14102564, 0.15      , 0.06779661,\n",
      "       0.05217391, 0.08791209, 0.06896552, 0.08333333, 0.14379085,\n",
      "       0.16      , 0.0733945 , 0.05673759, 0.07317073, 0.09230769,\n",
      "       0.02836879, 0.09150327, 0.        , 0.12389381, 0.09954751,\n",
      "       0.09333333, 0.12598425, 0.10810811, 0.07042254, 0.        ,\n",
      "       0.01526718, 0.08247423, 0.12195122, 0.02631579, 0.35294118,\n",
      "       0.15116279, 0.08163265, 0.06866953, 0.07655502, 0.31055901,\n",
      "       0.43589744]), 'Cora_L1_XW': array([0.0246085 , 0.01179245, 0.02580645, 0.03406814, 0.03171247,\n",
      "       0.00665188, 0.01014199, 0.01603206, 0.02526316, 0.00559701,\n",
      "       0.00867679, 0.02025783, 0.01286765, 0.01446281, 0.02313167,\n",
      "       0.00417537, 0.00990099, 0.01260504, 0.03094778, 0.01446655,\n",
      "       0.00990099, 0.00884956, 0.01320755, 0.01577909, 0.01996008,\n",
      "       0.023766  , 0.02367942, 0.03386454, 0.00361011, 0.01351351,\n",
      "       0.00572519, 0.01030928, 0.02155172, 0.01073345, 0.01577909,\n",
      "       0.01059322, 0.01515152, 0.00406504, 0.02830189, 0.00742115,\n",
      "       0.0268714 , 0.01949318, 0.05758157, 0.03846154, 0.01929825,\n",
      "       0.06722689, 0.02957486, 0.03797468, 0.03639847, 0.03269231,\n",
      "       0.01548673, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        ]), 'Pubmed_L1_AXW': array([0.43986254, 0.05673759, 0.10204082, 0.02890173, 0.07438017,\n",
      "       0.04268293, 0.02970297, 0.02359882, 0.04713115, 0.04790419,\n",
      "       0.03412969, 0.        , 0.01724138, 0.0060241 , 0.00671141,\n",
      "       0.01960784, 0.00806452, 0.00706714, 0.02824859, 0.06427221,\n",
      "       0.04263566, 0.02083333, 0.00645161, 0.06392694, 0.02024291,\n",
      "       0.04712042, 0.00566572, 0.01570681, 0.02228412, 0.02179837,\n",
      "       0.01587302, 0.1134402 , 0.04280156, 0.05088063, 0.01558442,\n",
      "       0.        , 0.00886918, 0.02074689, 0.03090508, 0.06231454,\n",
      "       0.04081633, 0.01687764, 0.02721088, 0.03658537, 0.06426485,\n",
      "       0.0233298 , 0.03582719, 0.01184433, 0.01118881, 0.05758583,\n",
      "       0.02869955, 0.0894526 , 0.07332491, 0.09069494, 0.09004237,\n",
      "       0.05527638, 0.03246753, 0.00554785, 0.12340967, 0.03908795,\n",
      "       0.06204556, 0.02087287, 0.04401294, 0.03167899, 0.05149502,\n",
      "       0.07586207, 0.07541478, 0.06728704, 0.20189274, 0.0505618 ,\n",
      "       0.0212766 , 0.05008945, 0.05073996, 0.06119951, 0.01305057,\n",
      "       0.00881057, 0.05942857, 0.0140647 , 0.00317965, 0.06451613,\n",
      "       0.13227017, 0.14361485, 0.06585136, 0.10836278, 0.03446262,\n",
      "       0.09185804, 0.10133955, 0.03143093, 0.0248566 , 0.02281989,\n",
      "       0.07863696, 0.08974359, 0.03489848, 0.04461538, 0.07836991,\n",
      "       0.22051282]), 'Pubmed_L2_AXW': array([0.43986254, 0.05673759, 0.10204082, 0.02890173, 0.07438017,\n",
      "       0.04268293, 0.02970297, 0.02359882, 0.04713115, 0.04790419,\n",
      "       0.03412969, 0.        , 0.01724138, 0.0060241 , 0.00671141,\n",
      "       0.01960784, 0.00806452, 0.00706714, 0.02824859, 0.06427221,\n",
      "       0.04263566, 0.02083333, 0.00645161, 0.06392694, 0.02024291,\n",
      "       0.04712042, 0.00566572, 0.01570681, 0.02228412, 0.02179837,\n",
      "       0.01587302, 0.1134402 , 0.04280156, 0.05088063, 0.01558442,\n",
      "       0.        , 0.00886918, 0.02074689, 0.03090508, 0.06231454,\n",
      "       0.04081633, 0.01687764, 0.02721088, 0.03658537, 0.06426485,\n",
      "       0.0233298 , 0.03582719, 0.01184433, 0.01118881, 0.05758583,\n",
      "       0.02869955, 0.0894526 , 0.07332491, 0.09069494, 0.09004237,\n",
      "       0.05527638, 0.03246753, 0.00554785, 0.12340967, 0.03908795,\n",
      "       0.06204556, 0.02087287, 0.04401294, 0.03167899, 0.05149502,\n",
      "       0.07586207, 0.07541478, 0.06728704, 0.20189274, 0.0505618 ,\n",
      "       0.0212766 , 0.05008945, 0.05073996, 0.06119951, 0.01305057,\n",
      "       0.00881057, 0.05942857, 0.0140647 , 0.00317965, 0.06451613,\n",
      "       0.13227017, 0.14361485, 0.06585136, 0.10836278, 0.03446262,\n",
      "       0.09185804, 0.10133955, 0.03143093, 0.0248566 , 0.02281989,\n",
      "       0.07863696, 0.08974359, 0.03489848, 0.04461538, 0.07836991,\n",
      "       0.22051282]), 'Reddit_L1_AXW': array([0.09731812, 0.10899053, 0.13686728, 0.17225058, 0.13482494,\n",
      "       0.2843011 , 0.14385121, 0.18847551, 0.08982769, 0.22497263,\n",
      "       0.14677043, 0.12676791, 0.30466082, 0.14605152, 0.08247079,\n",
      "       0.15885234, 0.12072382, 0.15324923, 0.06228544, 0.07553389,\n",
      "       0.0973646 , 0.05808677, 0.30417414, 0.0929281 , 0.14745017,\n",
      "       0.09441619, 0.0680597 , 0.13478311, 0.10354731, 0.0667818 ,\n",
      "       0.14215398, 0.09201112, 0.07598618, 0.17467374, 0.05572609,\n",
      "       0.09987571, 0.07523244, 0.13199063, 0.12383601, 0.13253601,\n",
      "       0.05723184, 0.0737217 , 0.06477241, 0.08048422, 0.24411408,\n",
      "       0.0918216 , 0.11504111, 0.15859076, 0.27087933, 0.11624985,\n",
      "       0.22405207, 0.22091244, 0.32974516, 0.07426008, 0.10075375,\n",
      "       0.12912208, 0.21757051, 0.13458066, 0.08182301, 0.06930345,\n",
      "       0.11378038, 0.1275412 , 0.14285257, 0.08329643, 0.10528341,\n",
      "       0.09045091, 0.15070551, 0.11854009, 0.14744943, 0.10078444,\n",
      "       0.09518433, 0.10241122, 0.13702061, 0.10241874, 0.08090298,\n",
      "       0.15195725, 0.10658616, 0.10605809, 0.10562316, 0.08606635,\n",
      "       0.09540186, 0.10530692, 0.11132088, 0.08089657, 0.08210416,\n",
      "       0.08068217, 0.10215132, 0.07650087, 0.07221788, 0.09114358,\n",
      "       0.07983146, 0.06751684, 0.06411364, 0.06252353, 0.1485414 ,\n",
      "       0.08228271]), 'Reddit_L2_AXW': array([0.09731812, 0.10899053, 0.13686728, 0.17225058, 0.13482494,\n",
      "       0.2843011 , 0.14385121, 0.18847551, 0.08982769, 0.22497263,\n",
      "       0.14677043, 0.12676791, 0.30466082, 0.14605152, 0.08247079,\n",
      "       0.15885234, 0.12072382, 0.15324923, 0.06228544, 0.07553389,\n",
      "       0.0973646 , 0.05808677, 0.30417414, 0.0929281 , 0.14745017,\n",
      "       0.09441619, 0.0680597 , 0.13478311, 0.10354731, 0.0667818 ,\n",
      "       0.14215398, 0.09201112, 0.07598618, 0.17467374, 0.05572609,\n",
      "       0.09987571, 0.07523244, 0.13199063, 0.12383601, 0.13253601,\n",
      "       0.05723184, 0.0737217 , 0.06477241, 0.08048422, 0.24411408,\n",
      "       0.0918216 , 0.11504111, 0.15859076, 0.27087933, 0.11624985,\n",
      "       0.22405207, 0.22091244, 0.32974516, 0.07426008, 0.10075375,\n",
      "       0.12912208, 0.21757051, 0.13458066, 0.08182301, 0.06930345,\n",
      "       0.11378038, 0.1275412 , 0.14285257, 0.08329643, 0.10528341,\n",
      "       0.09045091, 0.15070551, 0.11854009, 0.14744943, 0.10078444,\n",
      "       0.09518433, 0.10241122, 0.13702061, 0.10241874, 0.08090298,\n",
      "       0.15195725, 0.10658616, 0.10605809, 0.10562316, 0.08606635,\n",
      "       0.09540186, 0.10530692, 0.11132088, 0.08089657, 0.08210416,\n",
      "       0.08068217, 0.10215132, 0.07650087, 0.07221788, 0.09114358,\n",
      "       0.07983146, 0.06751684, 0.06411364, 0.06252353, 0.1485414 ,\n",
      "       0.08228271]), 'Citeseer_L2_XW': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Citeseer_L1_XW': array([0.01057269, 0.00747664, 0.00369004, 0.01004566, 0.01826484,\n",
      "       0.00931966, 0.00436681, 0.01190476, 0.00465983, 0.00384246,\n",
      "       0.00368664, 0.00648148, 0.00485909, 0.00595238, 0.00639854,\n",
      "       0.00658514, 0.01901902, 0.01537071, 0.01111111, 0.00096712,\n",
      "       0.00091241, 0.01030928, 0.01037344, 0.01417769, 0.01658986,\n",
      "       0.01812689, 0.0056926 , 0.01035782, 0.00502513, 0.01073171,\n",
      "       0.01658986, 0.01541251, 0.01252408, 0.        , 0.00801603,\n",
      "       0.00998004, 0.01119403, 0.00729927, 0.00831025, 0.01064774,\n",
      "       0.00468165, 0.00383877, 0.00651163, 0.00434028, 0.00794351,\n",
      "       0.00722674, 0.00834879, 0.0204461 , 0.00569801, 0.00652985,\n",
      "       0.002849  , 0.00439367, 0.01074219, 0.00777454, 0.01329535,\n",
      "       0.01313321, 0.01046622, 0.00484966, 0.01203704, 0.00461255,\n",
      "       0.00733945, 0.01330967, 0.00368324, 0.0092081 , 0.00744879,\n",
      "       0.01164875, 0.00729927, 0.01962533, 0.01077199, 0.00720721,\n",
      "       0.00562852, 0.00813743, 0.00272232, 0.00979432, 0.02166065,\n",
      "       0.02426343, 0.01624549, 0.00764088, 0.01302326, 0.00746269,\n",
      "       0.02421525, 0.01086957, 0.00366636, 0.0055607 , 0.0044964 ,\n",
      "       0.01061008, 0.00873362, 0.00820419, 0.00956023, 0.00966608,\n",
      "       0.00526778, 0.00688299, 0.00451671, 0.00609756, 0.00749064,\n",
      "       0.01203611]), 'Citeseer_L1_AXW': array([1.        , 0.96825397, 0.96875   , 0.98      , 0.96226415,\n",
      "       0.925     , 0.94915254, 0.95121951, 0.95744681, 0.97435897,\n",
      "       0.97959184, 0.97619048, 1.        , 1.        , 0.97674419,\n",
      "       0.98214286, 0.95      , 0.96428571, 0.98113208, 0.98305085,\n",
      "       1.        , 0.92      , 0.92307692, 0.94871795, 0.95081967,\n",
      "       0.90322581, 0.93243243, 0.97560976, 0.97727273, 0.96078431,\n",
      "       0.97777778, 0.96078431, 0.92682927, 0.96296296, 0.91304348,\n",
      "       0.8030303 , 0.36666667, 0.08928571, 0.15068493, 0.3364486 ,\n",
      "       0.23376623, 0.09259259, 0.2688172 , 0.25757576, 0.03174603,\n",
      "       0.20481928, 0.1322314 , 0.12048193, 0.06349206, 0.08108108,\n",
      "       0.06451613, 0.06930693, 0.20588235, 0.15584416, 0.11538462,\n",
      "       0.03191489, 0.15204678, 0.02597403, 0.11009174, 0.2027027 ,\n",
      "       0.02222222, 0.17763158, 0.04477612, 0.06060606, 0.13207547,\n",
      "       0.09782609, 0.09375   , 0.25      , 0.12173913, 0.11214953,\n",
      "       0.05714286, 0.22222222, 0.09333333, 0.10285714, 0.12820513,\n",
      "       0.16535433, 0.30041152, 0.19130435, 0.12676056, 0.08938547,\n",
      "       0.14673913, 0.09032258, 0.15577889, 0.05      , 0.09302326,\n",
      "       0.08045977, 0.12056738, 0.15217391, 0.02564103, 0.17989418,\n",
      "       0.08695652, 0.05882353, 0.04651163, 0.17307692, 0.09615385,\n",
      "       0.175     ]), 'Citeseer_L2_AXW': array([1.        , 0.96825397, 0.96875   , 0.98      , 0.96226415,\n",
      "       0.925     , 0.94915254, 0.95121951, 0.95744681, 0.97435897,\n",
      "       0.97959184, 0.97619048, 1.        , 1.        , 0.97674419,\n",
      "       0.98214286, 0.95      , 0.96428571, 0.98113208, 0.98305085,\n",
      "       1.        , 0.92      , 0.92307692, 0.94871795, 0.95081967,\n",
      "       0.90322581, 0.93243243, 0.97560976, 0.97727273, 0.96078431,\n",
      "       0.97777778, 0.96078431, 0.92682927, 0.96296296, 0.91304348,\n",
      "       0.8030303 , 0.36666667, 0.08928571, 0.15068493, 0.3364486 ,\n",
      "       0.23376623, 0.09259259, 0.2688172 , 0.25757576, 0.03174603,\n",
      "       0.20481928, 0.1322314 , 0.12048193, 0.06349206, 0.08108108,\n",
      "       0.06451613, 0.06930693, 0.20588235, 0.15584416, 0.11538462,\n",
      "       0.03191489, 0.15204678, 0.02597403, 0.11009174, 0.2027027 ,\n",
      "       0.02222222, 0.17763158, 0.04477612, 0.06060606, 0.13207547,\n",
      "       0.09782609, 0.09375   , 0.25      , 0.12173913, 0.11214953,\n",
      "       0.05714286, 0.22222222, 0.09333333, 0.10285714, 0.12820513,\n",
      "       0.16535433, 0.30041152, 0.19130435, 0.12676056, 0.08938547,\n",
      "       0.14673913, 0.09032258, 0.15577889, 0.05      , 0.09302326,\n",
      "       0.08045977, 0.12056738, 0.15217391, 0.02564103, 0.17989418,\n",
      "       0.08695652, 0.05882353, 0.04651163, 0.17307692, 0.09615385,\n",
      "       0.175     ]), 'Pubmed_L2_XW': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Reddit_L2_XW': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Pubmed_L1_XW': array([0.66160815, 0.26588861, 0.06997333, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        ]), 'Reddit_L1_XW': array([0.66160815, 0.26588861, 0.06997333, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        ])}\n"
     ]
    }
   ],
   "source": [
    "# dict_packet_length, ratio_diag = get_packetLength_per_rank(num_rank)\n",
    "# print(dict_packet_length)\n",
    "# print('-------------------------------------------')\n",
    "# print(ratio_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01df027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constant import *\n",
    "import math\n",
    "import random as ra\n",
    "\n",
    "\n",
    "def sweep(sweep_threshold, sweep_step, payload_length, num_port):\n",
    "    # payload_length is a list and is in Bytes\n",
    "    ra.seed(1)\n",
    "    results = []\n",
    "    time_stamp = [0 for j in range(num_port)]\n",
    "    sweep_num = int(sweep_threshold/sweep_step)\n",
    "    num_expr = sweep_num ** num_port\n",
    "    verbose_freq = 100 # showing the progress for each segment\n",
    "    verbose_segment = int(num_expr/verbose_freq)\n",
    "    verbose_num = 0\n",
    "    for i in range(num_expr):\n",
    "        # print(i)\n",
    "        if i%verbose_segment == 0:\n",
    "            print(verbose_num, \" percent\")\n",
    "            verbose_num += 1\n",
    "        # print(time_stamp)\n",
    "        delay = comp_delay(time_stamp, payload_length, num_port, 'default', 0)\n",
    "        results.append(delay)\n",
    "        # print(delay)\n",
    "        time_stamp[0] += sweep_step\n",
    "        for k in range(num_port):\n",
    "            if time_stamp[k] == sweep_threshold:\n",
    "                time_stamp[k] = 0\n",
    "                if k != num_port - 1:\n",
    "                    time_stamp[k+1] += sweep_step\n",
    "    return results\n",
    "\n",
    "def sweepRand(num_expr, sweep_threshold, sweep_step, payload_length, num_port):\n",
    "    # payload_length is a list and is in Bytes\n",
    "    ra.seed(1)\n",
    "    results = []\n",
    "    sweep_num = int(sweep_threshold/sweep_step)\n",
    "    verbose_freq = 100 # showing the progress for each segment\n",
    "    verbose_segment = int(num_expr/verbose_freq)\n",
    "    verbose_num = 0\n",
    "    for i in range(num_expr):\n",
    "        # print(i)\n",
    "        if i%verbose_segment == 0:\n",
    "            print(verbose_num, \" percent\")\n",
    "            verbose_num += 1\n",
    "        # print(time_stamp)\n",
    "        # time_stamp[0] += sweep_step\n",
    "        time_stamp = [ra.randrange(0, sweep_threshold, sweep_step) for j in range(num_port)]\n",
    "        delay = comp_delay(time_stamp, payload_length, num_port, 'default', [0 for j in range(num_port)])\n",
    "        results.append(delay)\n",
    "        # print(delay)\n",
    "    print('finished')\n",
    "    return results\n",
    "\n",
    "def comp_delay(time_stamp, payload_length, num_port, op_type, inp_packet_delay):\n",
    "    # time is considered as ns\n",
    "    # time_stamp is a list\n",
    "    # payload_length is a list and is in Bytes\n",
    "    # inp_packet_delay is a list\n",
    "    mytime = 0\n",
    "    end_time_port = [0 for i in range(num_port)]\n",
    "    curr_queue = 0 # pointer to which queue\n",
    "    num_beats = [math.ceil((payload_length[k] + HEADER_LENGTH)/PHIT_SIZE) for k in range(num_port)]\n",
    "    if op_type == 'default':\n",
    "        packet_delay = [num_beats[k] * PERIOD_CLK for k in range(num_port)]\n",
    "    else:\n",
    "        packet_delay = inp_packet_delay\n",
    "    queuefinished = [False for i in range(num_port)]\n",
    "    # num_finished = 0\n",
    "    \n",
    "    while all(queuefinished) == False:\n",
    "        if mytime >= time_stamp[curr_queue] and queuefinished[curr_queue] == False: #schedule an consume the packet\n",
    "            queuefinished[curr_queue] = True\n",
    "            mytime += packet_delay[curr_queue]\n",
    "            end_time_port[curr_queue] = mytime + SWITCH_LATENCY\n",
    "            # print(\"queue id \", curr_queue, \" is finished\", \"num_queue finished is: \", num_finished, \"out of: \", num_port)\n",
    "            # num_finished += 1\n",
    "        \n",
    "        # round-robin\n",
    "        if (curr_queue == num_port - 1):\n",
    "            curr_queue = 0\n",
    "        else:\n",
    "            curr_queue += 1\n",
    "        \n",
    "        # one clock delay for checking the next queue\n",
    "        mytime += PERIOD_CLK \n",
    "    \n",
    "    end_time_global = mytime + SWITCH_LATENCY\n",
    "    return end_time_global, end_time_port\n",
    "               \n",
    "def analysis(results):\n",
    "    # print(results)\n",
    "    print('max: ', max(results))\n",
    "    print('min: ', min(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ca5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_port = 16\n",
    "# results = sweepRand(num_expr = 1000000, sweep_threshold = 100, sweep_step = 10,\n",
    "#                     payload_length = [1000 for k in range(num_port)], num_port = num_port)\n",
    "# analysis(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "837571be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constant import *\n",
    "def instruction_count(instr, vlen):\n",
    "    if instr == 'vse32_v':\n",
    "        return vlen + 6\n",
    "    elif instr == 'vle32_v':\n",
    "        return vlen + 6\n",
    "    elif instr == 'vmacc_xv': # dense\n",
    "        return vlen + PE_latency\n",
    "    elif instr == 'vmacc_vx': # dense\n",
    "        return vlen + PE_latency\n",
    "    elif instr == 'spvmacc_xv': # sparse\n",
    "        #     return vlen + PE_latency # lets factor out vlen and lump it with all off-diag nnz and instead only measure the overhead\n",
    "        return 1 + PE_LATENCY # 1 is b/c of packet_length at the begining of packet\n",
    "    elif instr == 'spvmacc_vx': # spearse\n",
    "#         return vlen + PE_LATENCY # lets factor out vlen and lump it with all off-diag nnz and instead only measure the overhead\n",
    "        return 1 + PE_LATENCY # 1 is b/c of packet_length at the begining of packet   \n",
    "    elif instr == 'vstreamout_v':\n",
    "        return vlen * NUM_COL\n",
    "    elif instr == 'vsetivli':\n",
    "        return 1\n",
    "    elif instr == 'wfi':\n",
    "        return 1\n",
    "    elif (instr == 'addi' or instr == 'lui' or instr == 'add' or instr == 'bne'): # scalar\n",
    "        return 1\n",
    "    else:\n",
    "        print('unsupported instruction')\n",
    "\n",
    "def gcn_FPGA_time_cgra(instruction, packet_length, SIMD_under_utilization):\n",
    "    num_rank = len(packet_length)\n",
    "    cgra_time = [0 for i in range(num_rank)]\n",
    "    for i in range(num_rank):\n",
    "        if packet_length[i] == 0:\n",
    "            cgra_time[i] = 0 # we should bypass cgra\n",
    "        else:\n",
    "            time_instr = 0 # init to zero\n",
    "            for key in instruction:\n",
    "                instr_vlen = instruction[key][0]\n",
    "                instr_cnt = instruction[key][1] # instruction count\n",
    "                instr_name = key.split('-')[0]\n",
    "                time_instr += (instruction_count(instr_name, instr_vlen))*instr_cnt\n",
    "            cgra_time[i] = (int(packet_length[i]/(SIMD_DEGREE*SIMD_under_utilization)) + time_instr) *PERIOD_CLK *1e-9 # in ns\n",
    "    return cgra_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab3c0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "GCN_matrices/Cora_feat_L2.txt\n",
      "GCN_matrices/Cora_A.txt\n",
      "GCN_matrices/Cora_feat_L1.txt\n",
      "GCN_matrices/Pubmed_A.txt\n",
      "GCN_matrices/Reddit_A.txt\n",
      "GCN_matrices/Citeseer_feat_L2.txt\n",
      "GCN_matrices/Citeseer_feat_L1.txt\n",
      "GCN_matrices/Citeseer_A.txt\n",
      "GCN_matrices/Pubmed_feat_L2.txt\n",
      "GCN_matrices/Reddit_feat_L2.txt\n",
      "GCN_matrices/Pubmed_feat_L1.txt\n",
      "GCN_matrices/Reddit_feat_L1.txt\n",
      "--------- final result -----------\n",
      "cpu results:  {'Cora_L1_XW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Cora_L1_AXW': [0.818438, 0.8669720000000001, 0.9306409999999999, 0.838577, 0.808566, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Cora_L2_XW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Cora_L2_AXW': [0.435747, 0.427726, 0.441889, 0.421052, 0.43496, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Citeseer_L1_XW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Citeseer_L1_AXW': [0.530435, 0.528549, 0.536424, 0.545272, 0.559416, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Citeseer_L2_XW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Citeseer_L2_AXW': [0.464706, 0.456493, 0.477418, 0.470645, 0.468149, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Pubmed_L1_XW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Pubmed_L1_AXW': [3.13037, 3.14136, 3.25325, 3.2856899999999998, 3.11322, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Pubmed_L2_XW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Pubmed_L2_AXW': [0.941799, 0.940021, 0.967635, 1.04159, 1.0194699999999999, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Reddit_L1_XW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Reddit_L1_AXW': [266.754, 264.392, 267.639, 265.6, 267.048, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Reddit_L2_XW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Reddit_L2_AXW': [654.393, 655.9509999999999, 655.327, 654.0820000000001, 661.126, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "fpga results:  {'Cora_L1_AXW': [0.003914000914669999, 0.00402300091467, 0.00393900091467, 0.0040260009146699995, 0.00411600091467], 'Cora_L2_AXW': [0.007578689713456414, 0.0026500009146699995, 0.0027110009146699998, 0.0026510009146699996, 0.00275500091467], 'Citeseer_L1_AXW': [0.0034420008947979998, 0.0036719596828814595, 0.01248522657051834, 0.0033240008947979997, 0.0034378125194280003], 'Citeseer_L2_AXW': [0.002267000894798, 0.0025112929127668188, 0.0022490008947979997, 0.002236000894798, 0.0023900008947979998], 'Pubmed_L1_AXW': [0.012321795810032872, 0.013711908630545692, 0.012514303502340562, 0.01241308811772518, 0.012494677861314921], 'Pubmed_L2_AXW': [0.003617934271571333, 0.0036291804254174865, 0.003518482989520051, 0.0035411958100328715, 0.00571523170746877], 'Reddit_L1_AXW': [38.326362964889405, 38.07740757443319, 38.53016176900988, 38.21421420606217, 38.44950378749573], 'Reddit_L2_AXW': [94.28398883620319, 94.53264714385459, 94.41188298368697, 94.24106037090377, 95.27178916229737]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "verbose = False\n",
    "total_rank = 96\n",
    "num_iter = 5\n",
    "TM = 1\n",
    "TK = 2\n",
    "vmacc_unroll_factor = 14\n",
    "vmacc_vlen = 1354\n",
    "SIMD_under_utilization = 1.0\n",
    "\n",
    "def get_results(total_rank, num_iter, TM, TK, vmacc_unroll_factor, vmacc_vlen, SIMD_under_utilization)\n",
    "    datasets = ['Cora', 'Citeseer', 'Pubmed', 'Reddit']\n",
    "    # first element is vlen and second element is instruction count\n",
    "    instruction = {'addi-0':[0, 5],\n",
    "                   'lui-0':[0, 1],\n",
    "                   'wfi-0':[0, 1],\n",
    "                   # outer loop\n",
    "                   'vsetivli-0':[0, 2*TM],\n",
    "                   'vle32_v-0':[vmacc_unroll_factor, TM],\n",
    "                   'addi-1':[0, 2*TM],\n",
    "                   'bne-0':[0, TM],\n",
    "                   'vstreamout_v-0':[vmacc_unroll_factor, TM],\n",
    "                   # inner loop\n",
    "                   'vsetivli-1':[0, 2*TM*TK],\n",
    "                   'vle32_v-1':[vmacc_vlen, TM*TK],\n",
    "                   'add-0':[0, TM*TK],\n",
    "                   'addi-2':[0, TM*TK],\n",
    "                   'bne-1':[0, TM*TK],\n",
    "                   'spvmacc_xv-0':[vmacc_vlen, TM*TK*vmacc_unroll_factor]}\n",
    "\n",
    "    # This is the raw data, storing all of the ranks\n",
    "    my_dict_cpu_unroll = {datasets[0] + '_L1_XW':[[] for i in range(3*num_iter)], # tot_time_iter1, ..., tot_time_iter5, comm_iter1-5, ..., comp_iter1-5\n",
    "               datasets[0] + '_L1_AXW':[[] for i in range(3*num_iter)],\n",
    "               datasets[0] + '_L2_XW':[[] for i in range(3*num_iter)],\n",
    "               datasets[0] + '_L2_AXW':[[] for i in range(3*num_iter)],\n",
    "               datasets[1] + '_L1_XW':[[] for i in range(3*num_iter)],\n",
    "               datasets[1] + '_L1_AXW':[[] for i in range(3*num_iter)],\n",
    "               datasets[1] + '_L2_XW':[[] for i in range(3*num_iter)],\n",
    "               datasets[1] + '_L2_AXW':[[] for i in range(3*num_iter)],\n",
    "               datasets[2] + '_L1_XW':[[] for i in range(3*num_iter)],\n",
    "               datasets[2] + '_L1_AXW':[[] for i in range(3*num_iter)],\n",
    "               datasets[2] + '_L2_XW':[[] for i in range(3*num_iter)],\n",
    "               datasets[2] + '_L2_AXW':[[] for i in range(3*num_iter)],\n",
    "               datasets[3] + '_L1_XW':[[] for i in range(3*num_iter)],\n",
    "               datasets[3] + '_L1_AXW':[[] for i in range(3*num_iter)],\n",
    "               datasets[3] + '_L2_XW':[[] for i in range(3*num_iter)],\n",
    "               datasets[3] + '_L2_AXW':[[] for i in range(3*num_iter)]}\n",
    "\n",
    "    # This is the actual cpu result (max of all ranks)\n",
    "    my_dict_cpu = {datasets[0] + '_L1_XW':[0 for i in range(num_iter)], # tot_time_iter1, ..., tot_time_iter5, comm_iter1-5, ..., comp_iter1-5\n",
    "               datasets[0] + '_L1_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[0] + '_L2_XW':[0 for i in range(num_iter)],\n",
    "               datasets[0] + '_L2_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[1] + '_L1_XW':[0 for i in range(num_iter)],\n",
    "               datasets[1] + '_L1_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[1] + '_L2_XW':[0 for i in range(num_iter)],\n",
    "               datasets[1] + '_L2_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[2] + '_L1_XW':[0 for i in range(num_iter)],\n",
    "               datasets[2] + '_L1_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[2] + '_L2_XW':[0 for i in range(num_iter)],\n",
    "               datasets[2] + '_L2_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[3] + '_L1_XW':[0 for i in range(num_iter)],\n",
    "               datasets[3] + '_L1_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[3] + '_L2_XW':[0 for i in range(num_iter)],\n",
    "               datasets[3] + '_L2_AXW':[0 for i in range(num_iter)]}\n",
    "\n",
    "    my_dict_fpga = {datasets[0] + '_L1_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[0] + '_L2_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[1] + '_L1_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[1] + '_L2_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[2] + '_L1_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[2] + '_L2_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[3] + '_L1_AXW':[0 for i in range(num_iter)],\n",
    "               datasets[3] + '_L2_AXW':[0 for i in range(num_iter)]}\n",
    "\n",
    "    # assign directory\n",
    "    fd = './GCN_results/96/'  \n",
    "    for filenm in os.listdir(fd): # iterate over files in that directory\n",
    "        file_name = os.path.join(fd, filenm)\n",
    "        splitted_file_name = filenm.split(\"_\")\n",
    "        dataset = splitted_file_name[1]\n",
    "        layer = splitted_file_name[2]\n",
    "        iteration = int(splitted_file_name[3])\n",
    "        tot_rank = int(splitted_file_name[4].split(\".\")[0])\n",
    "        if iteration != 1: # ignore the first iteration\n",
    "            with open(file_name) as f:\n",
    "                f.readline() # ignore the first line\n",
    "                f.readline() # ignore the second line\n",
    "                lines = f.readlines()\n",
    "                for i in range(tot_rank): # xW\n",
    "                    splitted_lines = lines[i].split(\" \")\n",
    "                    rank = int(splitted_lines[0])\n",
    "                    tot_time = float(splitted_lines[2])\n",
    "                    comm_time = float(splitted_lines[3])\n",
    "                    comp_time = float(splitted_lines[4].strip())\n",
    "                    key = dataset + '_' + layer + '_' + 'XW' \n",
    "                    my_dict_cpu_unroll[key][(iteration-2)].append(tot_time) # -2 b/c the first valid iteration is 2\n",
    "                    my_dict_cpu_unroll[key][(iteration-2) + num_iter].append(comm_time)\n",
    "                    my_dict_cpu_unroll[key][(iteration-2) + num_iter*2].append(comp_time)\n",
    "    #                 if verbose == True:\n",
    "    #                     print('rank: ', rank)\n",
    "    #                     print('tot_time: ', tot_time) # tot_time\n",
    "    #                     print('comm_time: ', comm_time) # comm\n",
    "    #                     print('comp_time: ', comp_time) # comp\n",
    "                for i in range(tot_rank): # A(xW)\n",
    "                    j = i + tot_rank\n",
    "                    splitted_lines = lines[j].split(\" \")\n",
    "                    rank = int(splitted_lines[0])\n",
    "                    tot_time = float(splitted_lines[2])\n",
    "                    comm_time = float(splitted_lines[3])\n",
    "                    comp_time = float(splitted_lines[4].strip())\n",
    "                    key = dataset + '_' + layer + '_' + 'AXW' \n",
    "                    my_dict_cpu_unroll[key][(iteration-2)].append(tot_time) # -2 b/c the first valid iteration is 2\n",
    "                    my_dict_cpu_unroll[key][(iteration-2) + num_iter].append(comm_time)\n",
    "                    my_dict_cpu_unroll[key][(iteration-2) + num_iter*2].append(comp_time)\n",
    "    #                 if verbose == True:\n",
    "    #                     print('rank: ', rank)\n",
    "    #                     print('tot_time: ', tot_time) # tot_time\n",
    "    #                     print('comm_time: ', comm_time) # comm\n",
    "    #                     print('comp_time: ', comp_time) # comp\n",
    "                        # Automatically ignore the last line.\n",
    "\n",
    "    # now my_dict_cpu_unroll is ready!\n",
    "    # Lets calculate my_dict_cpu and my_dict_fpga\n",
    "    dict_packet_length, ratio_diag = get_packetLength_per_rank(total_rank) # This is the number of elements\n",
    "    for key in my_dict_cpu_unroll:\n",
    "        result_list = my_dict_cpu_unroll[key]\n",
    "        time_stamp = [[0 for h in range(total_rank)] for g in range(num_iter)]\n",
    "        print(key)\n",
    "        for j in range(3*num_iter):\n",
    "            if key[-3:] == 'AXW': # otherwise if its XW we will not accelerate it\n",
    "                if j < num_iter: # tot time calculate max(tot_time)\n",
    "                    max_result = max(result_list[j])\n",
    "                    my_dict_cpu[key][j%num_iter] = max_result * 1000.0 # in ms\n",
    "                elif j >= num_iter and j < 2*num_iter: # comm (we get the timestamp from here)\n",
    "                    max_result = max(result_list[j])\n",
    "                    time_stamp[j%num_iter] = max_result - np.array(result_list[j]) # save time_stamp for later\n",
    "                elif j >= 2*num_iter and j < 3*num_iter: # comp\n",
    "                    this_ratio_diag = np.array(ratio_diag[key])\n",
    "                    this_packet_length = np.array(dict_packet_length[key])\n",
    "                    this_result_list = np.array(result_list[j])\n",
    "                    result_comp_eff = this_result_list * this_ratio_diag # only diagonal\n",
    "                    max_result = max(result_comp_eff * 1000.0) # in ms \n",
    "                    min_result = min(result_comp_eff * 1000.0) # in ms\n",
    "                    packet_length = this_packet_length * (1.0 - this_ratio_diag) # only off-diagonal # This is the number of elements\n",
    "    #                 time_stamp = np.array(result_list[j]) - min_result\n",
    "                    gcn_fpga_time = gcn_FPGA_time_cgra(instruction, packet_length, SIMD_under_utilization)# depends on the rank b/c different ranks have different packet length\n",
    "                    # gcn_fpga_time is a list but switch_time_global is a single time\n",
    "                    switch_time_global, switch_time_port = comp_delay(time_stamp[j%num_iter], packet_length, total_rank, 'gcn', gcn_fpga_time)\n",
    "                    tot_fpga_time = max((result_comp_eff * 1000.0) + (np.array(switch_time_port) * 1e-6)) # in ms\n",
    "                    # 1000 and 1e-6 are for converting sec to ms and ns to ms respectively\n",
    "                    my_dict_fpga[key][j%num_iter] = tot_fpga_time # ms\n",
    "    #                 if verbose == True:\n",
    "    #                     print('max comp fpga: ', max_result)\n",
    "    #                     print('min comp fpga: ', min_result)\n",
    "            else: # XW\n",
    "                if j < num_iter: # tot time calculate max(tot_time)\n",
    "                    max_result = max(result_list[j])\n",
    "                    my_dict_cpu[key][j%num_iter] = max_result * 1000.0 # in ms\n",
    "\n",
    "    # now my_dict_cpu and my_dict_fpga are ready\n",
    "    print('--------- final result -----------')\n",
    "    print('cpu results: ', my_dict_cpu)\n",
    "    print('fpga results: ', my_dict_fpga)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfa0143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./GCN_results/192 192\n",
      "./GCN_results/1536 1536\n",
      "./GCN_results/1152 1152\n",
      "./GCN_results/384 384\n",
      "./GCN_results/96 96\n",
      "./GCN_results/768 768\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = './GCN_results/'\n",
    "for filename in os.listdir(directory):\n",
    "    fd = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isdir(fd) and fd.split('/')[-1] != '.ipynb_checkpoints':\n",
    "        tot_rank = int(fd.split('/')[-1])\n",
    "        print(fd, tot_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3dd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
