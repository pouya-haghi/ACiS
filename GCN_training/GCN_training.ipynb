{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7abdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "gcn_msg = fn.copy_u(u='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195c08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Creating a local scope so that all the stored ndata and edata\n",
    "        # (such as the `'h'` ndata below) are automatically popped out\n",
    "        # when the scope exits.\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = feature\n",
    "            g.update_all(gcn_msg, gcn_reduce)\n",
    "            h = g.ndata['h']\n",
    "            return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb063f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, L1_in_feats, L1_out_feats, L2_in_feats, L2_out_feats):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = GCNLayer(L1_in_feats, L1_out_feats)\n",
    "        self.layer2 = GCNLayer(L2_in_feats, L2_out_feats)\n",
    "#         self.verbose = verbose\n",
    "\n",
    "    def forward(self, g, features, verbose):\n",
    "        x1 = F.relu(self.layer1(g, features))\n",
    "        x2 = self.layer2(g, x1)\n",
    "        if verbose == True:\n",
    "#             print(x1)\n",
    "            return x1\n",
    "        else:\n",
    "            return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1707c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features, False)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f187649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset):\n",
    "    data_set = dataset\n",
    "    g = data_set[0]\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0fd173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(file_name, coo_array):\n",
    "    file = open(file_name, \"w\")\n",
    "    hidden_feature_size = len(coo_array.data)\n",
    "    print(str(coo_array.shape[0])+\" \"+str(coo_array.shape[1]), file=file)\n",
    "    print(hidden_feature_size, file=file)\n",
    "\n",
    "    for i in range(hidden_feature_size):\n",
    "        print(str(coo_array.row[i])+\" \"+str(coo_array.col[i])+\" \"+str(coo_array.data[i]), file=file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "458ed9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 00000 | Loss 1.9631 | Test Acc 0.1900 | Time(s) nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pouya/opt/anaconda3/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/Users/pouya/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/pouya/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 | Loss 1.8063 | Test Acc 0.4400 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.6556 | Test Acc 0.5420 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.5267 | Test Acc 0.5830 | Time(s) 0.0240\n",
      "Epoch 00004 | Loss 1.4080 | Test Acc 0.6150 | Time(s) 0.0241\n",
      "Epoch 00005 | Loss 1.3046 | Test Acc 0.6410 | Time(s) 0.0244\n",
      "Epoch 00006 | Loss 1.2102 | Test Acc 0.6730 | Time(s) 0.0243\n",
      "Epoch 00007 | Loss 1.1190 | Test Acc 0.6980 | Time(s) 0.0244\n",
      "Epoch 00008 | Loss 1.0296 | Test Acc 0.7240 | Time(s) 0.0244\n",
      "Epoch 00009 | Loss 0.9434 | Test Acc 0.7200 | Time(s) 0.0243\n",
      "Epoch 00010 | Loss 0.8620 | Test Acc 0.7200 | Time(s) 0.0243\n",
      "Epoch 00011 | Loss 0.7873 | Test Acc 0.7170 | Time(s) 0.0243\n",
      "Epoch 00012 | Loss 0.7201 | Test Acc 0.7100 | Time(s) 0.0243\n",
      "Epoch 00013 | Loss 0.6595 | Test Acc 0.7140 | Time(s) 0.0243\n",
      "Epoch 00014 | Loss 0.6032 | Test Acc 0.7190 | Time(s) 0.0243\n",
      "Epoch 00015 | Loss 0.5502 | Test Acc 0.7260 | Time(s) 0.0243\n",
      "Epoch 00016 | Loss 0.5010 | Test Acc 0.7270 | Time(s) 0.0243\n",
      "Epoch 00017 | Loss 0.4564 | Test Acc 0.7430 | Time(s) 0.0243\n",
      "Epoch 00018 | Loss 0.4156 | Test Acc 0.7460 | Time(s) 0.0243\n",
      "Epoch 00019 | Loss 0.3783 | Test Acc 0.7520 | Time(s) 0.0243\n",
      "Epoch 00020 | Loss 0.3443 | Test Acc 0.7510 | Time(s) 0.0243\n",
      "Epoch 00021 | Loss 0.3134 | Test Acc 0.7540 | Time(s) 0.0243\n",
      "Epoch 00022 | Loss 0.2854 | Test Acc 0.7530 | Time(s) 0.0244\n",
      "Epoch 00023 | Loss 0.2598 | Test Acc 0.7550 | Time(s) 0.0244\n",
      "Epoch 00024 | Loss 0.2366 | Test Acc 0.7560 | Time(s) 0.0244\n",
      "Epoch 00025 | Loss 0.2154 | Test Acc 0.7590 | Time(s) 0.0244\n",
      "Epoch 00026 | Loss 0.1960 | Test Acc 0.7580 | Time(s) 0.0244\n",
      "Epoch 00027 | Loss 0.1783 | Test Acc 0.7600 | Time(s) 0.0244\n",
      "Epoch 00028 | Loss 0.1622 | Test Acc 0.7600 | Time(s) 0.0244\n",
      "Epoch 00029 | Loss 0.1476 | Test Acc 0.7580 | Time(s) 0.0244\n",
      "Epoch 00030 | Loss 0.1343 | Test Acc 0.7590 | Time(s) 0.0245\n",
      "Epoch 00031 | Loss 0.1223 | Test Acc 0.7560 | Time(s) 0.0245\n",
      "Epoch 00032 | Loss 0.1114 | Test Acc 0.7530 | Time(s) 0.0245\n",
      "Epoch 00033 | Loss 0.1015 | Test Acc 0.7550 | Time(s) 0.0245\n",
      "Epoch 00034 | Loss 0.0925 | Test Acc 0.7530 | Time(s) 0.0245\n",
      "Epoch 00035 | Loss 0.0845 | Test Acc 0.7540 | Time(s) 0.0245\n",
      "Epoch 00036 | Loss 0.0772 | Test Acc 0.7550 | Time(s) 0.0245\n",
      "Epoch 00037 | Loss 0.0707 | Test Acc 0.7590 | Time(s) 0.0245\n",
      "Epoch 00038 | Loss 0.0649 | Test Acc 0.7590 | Time(s) 0.0245\n",
      "Epoch 00039 | Loss 0.0596 | Test Acc 0.7580 | Time(s) 0.0245\n",
      "Epoch 00040 | Loss 0.0548 | Test Acc 0.7560 | Time(s) 0.0245\n",
      "Epoch 00041 | Loss 0.0505 | Test Acc 0.7580 | Time(s) 0.0245\n",
      "Epoch 00042 | Loss 0.0467 | Test Acc 0.7580 | Time(s) 0.0245\n",
      "Epoch 00043 | Loss 0.0432 | Test Acc 0.7550 | Time(s) 0.0245\n",
      "Epoch 00044 | Loss 0.0400 | Test Acc 0.7540 | Time(s) 0.0245\n",
      "Epoch 00045 | Loss 0.0372 | Test Acc 0.7530 | Time(s) 0.0245\n",
      "Epoch 00046 | Loss 0.0346 | Test Acc 0.7530 | Time(s) 0.0245\n",
      "Epoch 00047 | Loss 0.0322 | Test Acc 0.7540 | Time(s) 0.0245\n",
      "Epoch 00048 | Loss 0.0300 | Test Acc 0.7530 | Time(s) 0.0245\n",
      "Epoch 00049 | Loss 0.0281 | Test Acc 0.7510 | Time(s) 0.0245\n"
     ]
    }
   ],
   "source": [
    "####################    Cora Dataset     #####################\n",
    "\n",
    "from dgl.data import CoraGraphDataset\n",
    "net = Net(1433, 16, 16, 7)\n",
    "print(net)\n",
    "g, features, labels, train_mask, test_mask = load_dataset(dataset = CoraGraphDataset())\n",
    "# Add edges between each node and itself to preserve old node representations\n",
    "g.add_edges(g.nodes(), g.nodes())\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dur = []\n",
    "num_epoch = 50\n",
    "for epoch in range(num_epoch):\n",
    "    t0 = time.time()\n",
    "\n",
    "    net.train()\n",
    "#     golden = net.layer1\n",
    "    logits = net(g, features, False) # forward\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    dur.append(time.time() - t0)\n",
    "    acc = evaluate(net, g, features, labels, test_mask) # forward\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))\n",
    "    \n",
    "hidden_feature = net(g, features, True)\n",
    "coo_hidden_feature = coo_matrix(hidden_feature.detach().numpy())\n",
    "###  save to file ###\n",
    "save_to_file(\"Cora_feat_L2.txt\", coo_hidden_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b94e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=3703, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "Downloading /Users/pouya/.dgl/citeseer.zip from https://data.dgl.ai/dataset/citeseer.zip...\n",
      "Extracting file to /Users/pouya/.dgl/citeseer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pouya/opt/anaconda3/lib/python3.9/site-packages/dgl/data/citation_graph.py:284: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "Epoch 00000 | Loss 1.8091 | Test Acc 0.2500 | Time(s) 0.0635\n",
      "Epoch 00001 | Loss 1.7154 | Test Acc 0.2770 | Time(s) 0.0633\n",
      "Epoch 00002 | Loss 1.6431 | Test Acc 0.3030 | Time(s) 0.0631\n",
      "Epoch 00003 | Loss 1.5790 | Test Acc 0.3570 | Time(s) 0.0631\n",
      "Epoch 00004 | Loss 1.5127 | Test Acc 0.4080 | Time(s) 0.0631\n",
      "Epoch 00005 | Loss 1.4440 | Test Acc 0.4440 | Time(s) 0.0632\n",
      "Epoch 00006 | Loss 1.3814 | Test Acc 0.4830 | Time(s) 0.0632\n",
      "Epoch 00007 | Loss 1.3262 | Test Acc 0.5280 | Time(s) 0.0633\n",
      "Epoch 00008 | Loss 1.2743 | Test Acc 0.5310 | Time(s) 0.0633\n",
      "Epoch 00009 | Loss 1.2221 | Test Acc 0.5330 | Time(s) 0.0634\n",
      "Epoch 00010 | Loss 1.1677 | Test Acc 0.5460 | Time(s) 0.0634\n",
      "Epoch 00011 | Loss 1.1115 | Test Acc 0.5450 | Time(s) 0.0634\n",
      "Epoch 00012 | Loss 1.0544 | Test Acc 0.5550 | Time(s) 0.0634\n",
      "Epoch 00013 | Loss 0.9970 | Test Acc 0.5750 | Time(s) 0.0633\n",
      "Epoch 00014 | Loss 0.9397 | Test Acc 0.5930 | Time(s) 0.0633\n",
      "Epoch 00015 | Loss 0.8834 | Test Acc 0.5870 | Time(s) 0.0633\n",
      "Epoch 00016 | Loss 0.8287 | Test Acc 0.5710 | Time(s) 0.0633\n",
      "Epoch 00017 | Loss 0.7758 | Test Acc 0.5680 | Time(s) 0.0633\n",
      "Epoch 00018 | Loss 0.7252 | Test Acc 0.5640 | Time(s) 0.0633\n",
      "Epoch 00019 | Loss 0.6766 | Test Acc 0.5680 | Time(s) 0.0633\n",
      "Epoch 00020 | Loss 0.6299 | Test Acc 0.5730 | Time(s) 0.0633\n",
      "Epoch 00021 | Loss 0.5852 | Test Acc 0.5740 | Time(s) 0.0633\n",
      "Epoch 00022 | Loss 0.5426 | Test Acc 0.5770 | Time(s) 0.0632\n",
      "Epoch 00023 | Loss 0.5025 | Test Acc 0.5780 | Time(s) 0.0632\n",
      "Epoch 00024 | Loss 0.4646 | Test Acc 0.5820 | Time(s) 0.0632\n",
      "Epoch 00025 | Loss 0.4289 | Test Acc 0.5850 | Time(s) 0.0632\n",
      "Epoch 00026 | Loss 0.3952 | Test Acc 0.5890 | Time(s) 0.0632\n",
      "Epoch 00027 | Loss 0.3634 | Test Acc 0.5950 | Time(s) 0.0632\n",
      "Epoch 00028 | Loss 0.3335 | Test Acc 0.6010 | Time(s) 0.0631\n",
      "Epoch 00029 | Loss 0.3057 | Test Acc 0.6080 | Time(s) 0.0632\n",
      "Epoch 00030 | Loss 0.2799 | Test Acc 0.6090 | Time(s) 0.0632\n",
      "Epoch 00031 | Loss 0.2560 | Test Acc 0.6090 | Time(s) 0.0631\n",
      "Epoch 00032 | Loss 0.2339 | Test Acc 0.6100 | Time(s) 0.0631\n",
      "Epoch 00033 | Loss 0.2136 | Test Acc 0.6120 | Time(s) 0.0631\n",
      "Epoch 00034 | Loss 0.1949 | Test Acc 0.6160 | Time(s) 0.0631\n",
      "Epoch 00035 | Loss 0.1778 | Test Acc 0.6170 | Time(s) 0.0631\n",
      "Epoch 00036 | Loss 0.1622 | Test Acc 0.6180 | Time(s) 0.0631\n",
      "Epoch 00037 | Loss 0.1479 | Test Acc 0.6180 | Time(s) 0.0631\n",
      "Epoch 00038 | Loss 0.1349 | Test Acc 0.6200 | Time(s) 0.0631\n",
      "Epoch 00039 | Loss 0.1231 | Test Acc 0.6230 | Time(s) 0.0631\n",
      "Epoch 00040 | Loss 0.1123 | Test Acc 0.6230 | Time(s) 0.0631\n",
      "Epoch 00041 | Loss 0.1024 | Test Acc 0.6250 | Time(s) 0.0631\n",
      "Epoch 00042 | Loss 0.0936 | Test Acc 0.6260 | Time(s) 0.0631\n",
      "Epoch 00043 | Loss 0.0856 | Test Acc 0.6270 | Time(s) 0.0631\n",
      "Epoch 00044 | Loss 0.0783 | Test Acc 0.6220 | Time(s) 0.0631\n",
      "Epoch 00045 | Loss 0.0718 | Test Acc 0.6230 | Time(s) 0.0631\n",
      "Epoch 00046 | Loss 0.0659 | Test Acc 0.6210 | Time(s) 0.0631\n",
      "Epoch 00047 | Loss 0.0606 | Test Acc 0.6170 | Time(s) 0.0631\n",
      "Epoch 00048 | Loss 0.0558 | Test Acc 0.6160 | Time(s) 0.0630\n",
      "Epoch 00049 | Loss 0.0514 | Test Acc 0.6160 | Time(s) 0.0631\n"
     ]
    }
   ],
   "source": [
    "####################    Citeseer Dataset     #####################\n",
    "\n",
    "from dgl.data import CiteseerGraphDataset\n",
    "net = Net(3703, 16, 16, 6)\n",
    "print(net)\n",
    "g, features, labels, train_mask, test_mask = load_dataset(dataset = CiteseerGraphDataset())\n",
    "# Add edges between each node and itself to preserve old node representations\n",
    "g.add_edges(g.nodes(), g.nodes())\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dur = []\n",
    "num_epoch = 50\n",
    "for epoch in range(num_epoch):\n",
    "    t0 = time.time()\n",
    "    net.train()\n",
    "#     golden = net.layer1\n",
    "    logits = net(g, features, False) # forward\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    dur.append(time.time() - t0)\n",
    "    acc = evaluate(net, g, features, labels, test_mask) # forward\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))\n",
    "    \n",
    "hidden_feature = net(g, features, True)\n",
    "coo_hidden_feature = coo_matrix(hidden_feature.detach().numpy())\n",
    "###  save to file ###\n",
    "save_to_file(\"Citeseer_feat_L2.txt\", coo_hidden_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d81e040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=500, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 00000 | Loss 1.1051 | Test Acc 0.5700 | Time(s) 0.0556\n",
      "Epoch 00001 | Loss 0.9994 | Test Acc 0.6160 | Time(s) 0.0539\n",
      "Epoch 00002 | Loss 0.8937 | Test Acc 0.6400 | Time(s) 0.0540\n",
      "Epoch 00003 | Loss 0.8126 | Test Acc 0.6660 | Time(s) 0.0535\n",
      "Epoch 00004 | Loss 0.7219 | Test Acc 0.6770 | Time(s) 0.0535\n",
      "Epoch 00005 | Loss 0.6516 | Test Acc 0.6890 | Time(s) 0.0532\n",
      "Epoch 00006 | Loss 0.5781 | Test Acc 0.6840 | Time(s) 0.0531\n",
      "Epoch 00007 | Loss 0.5191 | Test Acc 0.6900 | Time(s) 0.0530\n",
      "Epoch 00008 | Loss 0.4652 | Test Acc 0.7070 | Time(s) 0.0530\n",
      "Epoch 00009 | Loss 0.4168 | Test Acc 0.7200 | Time(s) 0.0529\n",
      "Epoch 00010 | Loss 0.3762 | Test Acc 0.7250 | Time(s) 0.0529\n",
      "Epoch 00011 | Loss 0.3414 | Test Acc 0.7340 | Time(s) 0.0529\n",
      "Epoch 00012 | Loss 0.3098 | Test Acc 0.7350 | Time(s) 0.0529\n",
      "Epoch 00013 | Loss 0.2808 | Test Acc 0.7400 | Time(s) 0.0531\n",
      "Epoch 00014 | Loss 0.2544 | Test Acc 0.7410 | Time(s) 0.0530\n",
      "Epoch 00015 | Loss 0.2300 | Test Acc 0.7450 | Time(s) 0.0530\n",
      "Epoch 00016 | Loss 0.2072 | Test Acc 0.7490 | Time(s) 0.0531\n",
      "Epoch 00017 | Loss 0.1859 | Test Acc 0.7470 | Time(s) 0.0530\n",
      "Epoch 00018 | Loss 0.1659 | Test Acc 0.7440 | Time(s) 0.0531\n",
      "Epoch 00019 | Loss 0.1472 | Test Acc 0.7520 | Time(s) 0.0530\n",
      "Epoch 00020 | Loss 0.1300 | Test Acc 0.7520 | Time(s) 0.0530\n",
      "Epoch 00021 | Loss 0.1143 | Test Acc 0.7530 | Time(s) 0.0530\n",
      "Epoch 00022 | Loss 0.1000 | Test Acc 0.7550 | Time(s) 0.0531\n",
      "Epoch 00023 | Loss 0.0874 | Test Acc 0.7590 | Time(s) 0.0530\n",
      "Epoch 00024 | Loss 0.0766 | Test Acc 0.7610 | Time(s) 0.0530\n",
      "Epoch 00025 | Loss 0.0673 | Test Acc 0.7630 | Time(s) 0.0530\n",
      "Epoch 00026 | Loss 0.0592 | Test Acc 0.7600 | Time(s) 0.0530\n",
      "Epoch 00027 | Loss 0.0521 | Test Acc 0.7630 | Time(s) 0.0530\n",
      "Epoch 00028 | Loss 0.0459 | Test Acc 0.7630 | Time(s) 0.0530\n",
      "Epoch 00029 | Loss 0.0405 | Test Acc 0.7660 | Time(s) 0.0530\n",
      "Epoch 00030 | Loss 0.0358 | Test Acc 0.7630 | Time(s) 0.0530\n",
      "Epoch 00031 | Loss 0.0316 | Test Acc 0.7590 | Time(s) 0.0530\n",
      "Epoch 00032 | Loss 0.0279 | Test Acc 0.7610 | Time(s) 0.0530\n",
      "Epoch 00033 | Loss 0.0248 | Test Acc 0.7620 | Time(s) 0.0530\n",
      "Epoch 00034 | Loss 0.0221 | Test Acc 0.7630 | Time(s) 0.0530\n",
      "Epoch 00035 | Loss 0.0197 | Test Acc 0.7650 | Time(s) 0.0530\n",
      "Epoch 00036 | Loss 0.0177 | Test Acc 0.7630 | Time(s) 0.0530\n",
      "Epoch 00037 | Loss 0.0159 | Test Acc 0.7640 | Time(s) 0.0530\n",
      "Epoch 00038 | Loss 0.0144 | Test Acc 0.7640 | Time(s) 0.0530\n",
      "Epoch 00039 | Loss 0.0130 | Test Acc 0.7640 | Time(s) 0.0530\n",
      "Epoch 00040 | Loss 0.0118 | Test Acc 0.7680 | Time(s) 0.0530\n",
      "Epoch 00041 | Loss 0.0108 | Test Acc 0.7670 | Time(s) 0.0530\n",
      "Epoch 00042 | Loss 0.0099 | Test Acc 0.7680 | Time(s) 0.0530\n",
      "Epoch 00043 | Loss 0.0091 | Test Acc 0.7680 | Time(s) 0.0530\n",
      "Epoch 00044 | Loss 0.0083 | Test Acc 0.7650 | Time(s) 0.0530\n",
      "Epoch 00045 | Loss 0.0077 | Test Acc 0.7640 | Time(s) 0.0530\n",
      "Epoch 00046 | Loss 0.0072 | Test Acc 0.7640 | Time(s) 0.0529\n",
      "Epoch 00047 | Loss 0.0067 | Test Acc 0.7620 | Time(s) 0.0530\n",
      "Epoch 00048 | Loss 0.0062 | Test Acc 0.7620 | Time(s) 0.0529\n",
      "Epoch 00049 | Loss 0.0058 | Test Acc 0.7620 | Time(s) 0.0530\n"
     ]
    }
   ],
   "source": [
    "####################    Pubmed Dataset     #####################\n",
    "\n",
    "from dgl.data import PubmedGraphDataset\n",
    "net = Net(500, 16, 16, 3)\n",
    "print(net)\n",
    "g, features, labels, train_mask, test_mask = load_dataset(dataset = PubmedGraphDataset())\n",
    "# Add edges between each node and itself to preserve old node representations\n",
    "g.add_edges(g.nodes(), g.nodes())\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dur = []\n",
    "num_epoch = 50\n",
    "for epoch in range(num_epoch):\n",
    "    t0 = time.time()\n",
    "    net.train()\n",
    "#     golden = net.layer1\n",
    "    logits = net(g, features, False) # forward\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    dur.append(time.time() - t0)\n",
    "    acc = evaluate(net, g, features, labels, test_mask) # forward\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))\n",
    "    \n",
    "hidden_feature = net(g, features, True)\n",
    "coo_hidden_feature = coo_matrix(hidden_feature.detach().numpy())\n",
    "###  save to file ###\n",
    "save_to_file(\"Pubmed_feat_L2.txt\", coo_hidden_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1091c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=602, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=41, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 00000 | Loss 62792.0469 | Test Acc 0.1574 | Time(s) 43.9891\n",
      "Epoch 00001 | Loss 168816.2500 | Test Acc 0.0286 | Time(s) 42.3227\n",
      "Epoch 00002 | Loss 171078.6250 | Test Acc 0.0330 | Time(s) 41.8067\n",
      "Epoch 00003 | Loss 129313.9844 | Test Acc 0.0734 | Time(s) 41.6736\n",
      "Epoch 00004 | Loss 60475.3789 | Test Acc 0.1772 | Time(s) 41.5081\n",
      "Epoch 00005 | Loss 25114.3281 | Test Acc 0.2316 | Time(s) 41.2634\n",
      "Epoch 00006 | Loss 5377.3677 | Test Acc 0.3165 | Time(s) 41.1445\n",
      "Epoch 00007 | Loss 3517.0352 | Test Acc 0.3524 | Time(s) 41.0212\n",
      "Epoch 00008 | Loss 2742.8533 | Test Acc 0.3680 | Time(s) 40.9631\n",
      "Epoch 00009 | Loss 2166.3064 | Test Acc 0.3596 | Time(s) 40.9303\n",
      "Epoch 00010 | Loss 1941.8131 | Test Acc 0.3687 | Time(s) 40.8515\n",
      "Epoch 00011 | Loss 1572.6862 | Test Acc 0.3691 | Time(s) 40.7757\n",
      "Epoch 00012 | Loss 1513.4431 | Test Acc 0.3773 | Time(s) 40.8183\n",
      "Epoch 00013 | Loss 1417.0891 | Test Acc 0.3879 | Time(s) 40.8011\n",
      "Epoch 00014 | Loss 1295.3516 | Test Acc 0.3900 | Time(s) 40.7439\n",
      "Epoch 00015 | Loss 1203.6027 | Test Acc 0.3867 | Time(s) 40.7118\n",
      "Epoch 00016 | Loss 1144.3143 | Test Acc 0.3220 | Time(s) 40.6697\n",
      "Epoch 00017 | Loss 1547.5653 | Test Acc 0.3656 | Time(s) 40.6241\n",
      "Epoch 00018 | Loss 1095.4663 | Test Acc 0.3678 | Time(s) 40.5939\n",
      "Epoch 00019 | Loss 1138.1344 | Test Acc 0.3458 | Time(s) 40.5671\n",
      "Epoch 00020 | Loss 1351.9807 | Test Acc 0.3871 | Time(s) 40.5585\n",
      "Epoch 00021 | Loss 1291.6205 | Test Acc 0.3921 | Time(s) 40.5614\n",
      "Epoch 00022 | Loss 1372.3303 | Test Acc 0.3937 | Time(s) 40.5407\n",
      "Epoch 00023 | Loss 1437.0015 | Test Acc 0.3856 | Time(s) 40.5356\n",
      "Epoch 00024 | Loss 1495.9401 | Test Acc 0.3910 | Time(s) 40.5206\n",
      "Epoch 00025 | Loss 1537.3209 | Test Acc 0.3951 | Time(s) 40.5061\n",
      "Epoch 00026 | Loss 1565.1802 | Test Acc 0.3991 | Time(s) 40.5070\n",
      "Epoch 00027 | Loss 1594.9960 | Test Acc 0.4024 | Time(s) 40.4887\n",
      "Epoch 00028 | Loss 1620.4979 | Test Acc 0.4059 | Time(s) 40.4789\n",
      "Epoch 00029 | Loss 1632.9017 | Test Acc 0.4091 | Time(s) 40.4845\n",
      "Epoch 00030 | Loss 1631.2074 | Test Acc 0.4117 | Time(s) 40.4762\n",
      "Epoch 00031 | Loss 1618.5940 | Test Acc 0.4128 | Time(s) 40.4733\n",
      "Epoch 00032 | Loss 1598.1204 | Test Acc 0.4138 | Time(s) 40.4792\n",
      "Epoch 00033 | Loss 1586.9132 | Test Acc 0.4146 | Time(s) 40.4721\n",
      "Epoch 00034 | Loss 1558.1390 | Test Acc 0.4154 | Time(s) 40.4558\n",
      "Epoch 00035 | Loss 1522.8547 | Test Acc 0.4141 | Time(s) 40.4425\n",
      "Epoch 00036 | Loss 1491.8882 | Test Acc 0.4120 | Time(s) 40.4343\n",
      "Epoch 00037 | Loss 1458.5535 | Test Acc 0.4108 | Time(s) 40.4227\n",
      "Epoch 00038 | Loss 1415.6534 | Test Acc 0.4064 | Time(s) 40.4115\n",
      "Epoch 00039 | Loss 1373.4453 | Test Acc 0.4060 | Time(s) 40.4016\n",
      "Epoch 00040 | Loss 1331.9398 | Test Acc 0.4063 | Time(s) 40.3910\n",
      "Epoch 00041 | Loss 1288.0215 | Test Acc 0.4080 | Time(s) 40.3817\n",
      "Epoch 00042 | Loss 1239.7861 | Test Acc 0.4078 | Time(s) 40.3752\n",
      "Epoch 00043 | Loss 1188.2777 | Test Acc 0.4069 | Time(s) 40.3727\n",
      "Epoch 00044 | Loss 1135.7809 | Test Acc 0.4066 | Time(s) 40.3735\n",
      "Epoch 00045 | Loss 1083.1229 | Test Acc 0.4013 | Time(s) 40.3665\n",
      "Epoch 00046 | Loss 1037.8903 | Test Acc 0.4080 | Time(s) 40.3592\n",
      "Epoch 00047 | Loss 982.2836 | Test Acc 0.4054 | Time(s) 40.3522\n",
      "Epoch 00048 | Loss 936.8096 | Test Acc 0.4019 | Time(s) 40.3448\n",
      "Epoch 00049 | Loss 894.2143 | Test Acc 0.4028 | Time(s) 40.3704\n"
     ]
    }
   ],
   "source": [
    "####################    Reddit Dataset     #####################\n",
    "\n",
    "from dgl.data import RedditDataset\n",
    "net = Net(602, 16, 16, 41)\n",
    "print(net)\n",
    "g, features, labels, train_mask, test_mask = load_dataset(dataset = RedditDataset())\n",
    "# Add edges between each node and itself to preserve old node representations\n",
    "g.add_edges(g.nodes(), g.nodes())\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dur = []\n",
    "num_epoch = 50\n",
    "for epoch in range(num_epoch):\n",
    "    t0 = time.time()\n",
    "    net.train()\n",
    "#     golden = net.layer1\n",
    "    logits = net(g, features, False) # forward\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    dur.append(time.time() - t0)\n",
    "    acc = evaluate(net, g, features, labels, test_mask) # forward\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))\n",
    "    \n",
    "hidden_feature = net(g, features, True)\n",
    "coo_hidden_feature = coo_matrix(hidden_feature.detach().numpy())\n",
    "###  save to file ###\n",
    "save_to_file(\"Reddit_feat_L2.txt\", coo_hidden_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3eb5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6 9.9 7.9 7.9]\n",
      "[0 0 1 1]\n",
      "[0 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy.sparse import coo_matrix\n",
    "a = np.array([[2.6, 0.0, 9.9, 0], [0.0, 0.0, 7.9, 7.9]])\n",
    "b = coo_matrix(a)\n",
    "print(b.data)\n",
    "print(b.row)\n",
    "print(b.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d95dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rdflib\n",
    "# import rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30fc25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
